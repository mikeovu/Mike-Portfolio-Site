
[{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS","type":"tags"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/categories/guides/","section":"Categories","summary":"","title":"Guides","type":"categories"},{"content":" What is a State File? # Terraform stores information about your infrastructure in a state file. This state file keeps track of resources created by your configuration and maps them to real-world resources.\nWhen you run terraform apply or terraform destroy against your initialized configuration, Terraform writes metadata about your configuration to the state file and updates your infrastructure resources accordingly.\nState File Project # In this project, we will create an AWS instance and security group, examine the project\u0026rsquo;s state file, and use Terraform to remove infrastructure from the project\u0026rsquo;s state.\nPrerequisites # Terraform CLI 1.7+ An AWS Account AWS CLI AWS credentials configured locally with your access keys and a default region. Create Infrastructure State # Clone the Learn Terraform State Management repository\n$ git clone git clone https://github.com/hashicorp/learn-terraform-state.git Change into the new directory:\n$ cd learn-terraform-state Review the main.tf file. This configuration deploys an Ubuntu EC2 instance publicly accessible on port 8080.\nprovider \u0026#34;aws\u0026#34; { region = var.aws_region } data \u0026#34;aws_ami\u0026#34; \u0026#34;ubuntu\u0026#34; { most_recent = true filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\u0026#34;] } filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } owners = [\u0026#34;099720109477\u0026#34;] # Canonical } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;sg_8080\u0026#34; { name = \u0026#34;terraform-learn-state-sg-8080\u0026#34; ingress { from_port = \u0026#34;8080\u0026#34; to_port = \u0026#34;8080\u0026#34; protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } // connectivity to ubuntu mirrors is required to run `apt-get update` and `apt-get install apache2` egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = data.aws_ami.ubuntu.id instance_type = \u0026#34;t2.micro\u0026#34; vpc_security_group_ids = [aws_security_group.sg_8080.id] user_data = \u0026lt;\u0026lt;-EOF #!/bin/bash apt-get update apt-get install -y apache2 sed -i -e \u0026#39;s/80/8080/\u0026#39; /etc/apache2/ports.conf echo \u0026#34;Hello World\u0026#34; \u0026gt; /var/www/html/index.html systemctl restart apache2 EOF tags = { Name = \u0026#34;terraform-learn-state-ec2\u0026#34; } } This configuration uses the AWS provider to create an EC2 instance and a security group that allows public access.\nInitialize the directory.\n$ terraform init Initializing the backend... Initializing provider plugins... - Reusing previous version of hashicorp/aws from the dependency lock file - Installing hashicorp/aws v5.31.0... - Installed hashicorp/aws v5.31.0 (signed by HashiCorp) Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. After Terraform initializes, apply the configuration and approve the run by typing yes at the prompt.\n$ terraform apply data.aws_ami.ubuntu: Reading... data.aws_ami.ubuntu: Read complete after 0s [id=ami-027a754129abb5386] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.example will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ##... Plan: 2 to add, 0 to change, 0 to destroy. Changes to Outputs: + aws_region = \u0026#34;us-east-1\u0026#34; + instance_id = (known after apply) + public_ip = (known after apply) + security_group = (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_security_group.sg_8080: Creating... aws_security_group.sg_8080: Creation complete after 3s [id=sg-0adfd0a0ade3eebdc] aws_instance.example: Creating... aws_instance.example: Still creating... [10s elapsed] aws_instance.example: Still creating... [20s elapsed] aws_instance.example: Still creating... [30s elapsed] aws_instance.example: Creation complete after 32s [id=i-05a8893f05c6a37be] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. Outputs: aws_region = \u0026#34;us-east-1\u0026#34; instance_id = \u0026#34;i-05a8893f05c6a37be\u0026#34; public_ip = \u0026#34;18.212.104.187\u0026#34; security_group = \u0026#34;sg-0adfd0a0ade3eebdc\u0026#34; Examine the state file # Now that you have applied this configuration, you have a local state file that tracks the resources Terraform created. Check your directory to confirm the terraform.tfstate file exists.\n$ ls -1 LICENSE README.md main.tf new_state outputs.tf terraform.tf terraform.tfstate variables.tf You should not manually change information in your state file in a real-world situation to avoid unnecessary drift between your Terraform configuration, state, and infrastructure. Any change in state could result in your infrastructure being destroyed and recreated at your next terraform apply.\nOpen the terraform.tfstate file in your file editor.\nThis example contains few resources, so your actual state file is relatively small.\nThis file is the JSON encoded state that Terraform writes and reads at each operation. The first stanza contains information about your Terraform application.\nExplore resources in state # The resources section of the state file contains the schema for any resources you create in Terraform. Review the resources section of this file.\n\u0026#34;resources\u0026#34;: [ { \u0026#34;mode\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;aws_ami\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ubuntu\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/aws\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 0, \u0026#34;attributes\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:ec2:us-east-1::image/ami-027a754129abb5386\u0026#34;, ##... }, ##... ] The first key in this schema is the mode. Mode refers to the type of resource Terraform creates — either a resource (managed) or a data source (data). The type key refers to the resource type - in this case, the aws_ami type is a resource available in the aws provider.\n##FIXME\n##... { \u0026#34;mode\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;aws_instance\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;example\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/aws\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 1, \u0026#34;attributes\u0026#34;: { \u0026#34;ami\u0026#34;: \u0026#34;ami-027a754129abb5386\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:ec2:us-east-1:949008909725:instance/i-05a8893f05c6a37be\u0026#34;, \u0026#34;associate_public_ip_address\u0026#34;: true, \u0026#34;availability_zone\u0026#34;: \u0026#34;us-east-1a\u0026#34;, ##... \u0026#34;public_ip\u0026#34;: \u0026#34;18.212.104.187\u0026#34;, ##... \u0026#34;secondary_private_ips\u0026#34;: [], \u0026#34;security_groups\u0026#34;: [ \u0026#34;terraform-learn-state-sg-8080\u0026#34; ], \u0026#34;source_dest_check\u0026#34;: true, \u0026#34;spot_instance_request_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;subnet_id\u0026#34;: \u0026#34;subnet-0e75b9376618c682a\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;terraform-learn-state-ec2\u0026#34; }, ##... } } ] }, The aws_instance type is a managed resource with the AMI from the data.aws_ami source.\nThe instances section in this resource contains the attributes of the resource. The security_groups attribute, for example, is captured in plain text in state as opposed to the variable interpolated string in the configuration file.\nTerraform also marks dependencies between resources in state with the built-in dependency tree logic.\n##... \u0026#34;dependencies\u0026#34;: [ \u0026#34;aws_security_group.sg_8080\u0026#34;, \u0026#34;data.aws_ami.ubuntu\u0026#34; ] ##... Because your state file has a record of your dependencies, enforced by you with a depends_on attribute or by Terraform automatically, any changes to the dependencies will force a change to the dependent resource.\n","date":"31 May 2024","externalUrl":null,"permalink":"/posts/learning-terraform/terraform-state/","section":"Posts \u0026 Projects","summary":"What is a State File?","title":"How to Manage Resources in Terraform State","type":"posts"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/tags/iac/","section":"Tags","summary":"","title":"IAC","type":"tags"},{"content":" Welcome! # In this page, I\u0026rsquo;ve added guides for projects I\u0026rsquo;ve worked on for learning tech.\n","date":"31 May 2024","externalUrl":null,"permalink":"/posts/","section":"Posts \u0026 Projects","summary":"Welcome!","title":"Posts \u0026 Projects","type":"posts"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"31 May 2024","externalUrl":null,"permalink":"/posts/learning-terraform/","section":"Posts \u0026 Projects","summary":"","title":"Terraform Learning Projects","type":"posts"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/cisco/","section":"Tags","summary":"","title":"Cisco","type":"tags"},{"content":" Save AWS Access Keys for Terraform Admin Account # Create an AWS credentials file so that Terraform can get the programmatic credentials for the AWS administrator pushing configs via Terraform. % mkdir ~/.aws/credentials\nNavigate to https://console.aws.amazon.com\nNavigate to IAM and search for Access Keys. Click on Manage Access Keys for admin user\nNavigate to the Access keys menu to generate the aws_access_key_id and aws_secret_access_key. Copy these keys.\nGo back to terminal and edit the ~/.aws/credentials file to paste the keys here:\n[default] aws_access_key_id = \u0026lt;access_key_id_here\u0026gt; aws_secret_access_key = \u0026lt;secret_access_key_here\u0026gt; Save the file. Use Terraform to Create an S3 bucket # Create a directory for the project. I created one called Configure_S3_Bucket_Terraform\nAdd a file called main.tf\n% mkdir S3_Configure_Bucket_Terraform % touch main.tf Paste the following code into main.tf and replace \u0026lt;your-name\u0026gt; with your name after the bucket argument provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-2\u0026#34; } resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;first_bucket\u0026#34; { bucket = \u0026#34;\u0026lt;your-name\u0026gt;-first-bucket\u0026#34; } Initialize Terraform from your directory: % terraform init\nOutput:\n(venv) host% Configure_S3_Bucket_Terraform % terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/aws... - Installing hashicorp/aws v5.49.0... - Installed hashicorp/aws v5.49.0 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026#34;terraform init\u0026#34; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Apply your Terraform configuration: % terraform apply\nType yes and hit enter (venv) host % Configure_S3_Bucket_Terraform % terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_s3_bucket.first_bucket will be created + resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;first_bucket\u0026#34; { + acceleration_status = (known after apply) + acl = (known after apply) + arn = (known after apply) + bucket = \u0026#34;mikes-first-bucket\u0026#34; + bucket_domain_name = (known after apply) + bucket_prefix = (known after apply) + bucket_regional_domain_name = (known after apply) + force_destroy = false + hosted_zone_id = (known after apply) + id = (known after apply) + object_lock_enabled = (known after apply) + policy = (known after apply) + region = (known after apply) + request_payer = (known after apply) + tags_all = (known after apply) + website_domain = (known after apply) + website_endpoint = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_s3_bucket.first_bucket: Creating... aws_s3_bucket.first_bucket: Creation complete after 2s [id=vus-first-bucket] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Verify Bucket Creation # Navigate to the Buckets menu of your AWS console.\nVerify that your Bucket was configured:\nTerraform Destroy # Run the % terraform destroy command\naws_s3_bucket.first_bucket: Refreshing state... [id=vus-first-bucket] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # aws_s3_bucket.first_bucket will be destroyed - resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;first_bucket\u0026#34; { - arn = \u0026#34;arn:aws:s3:::vus-first-bucket\u0026#34; -\u0026gt; null - bucket = \u0026#34;vus-first-bucket\u0026#34; -\u0026gt; null - bucket_domain_name = \u0026#34;vus-first-bucket.s3.amazonaws.com\u0026#34; -\u0026gt; null - bucket_regional_domain_name = \u0026#34;vus-first-bucket.s3.us-east-2.amazonaws.com\u0026#34; -\u0026gt; null - force_destroy = false -\u0026gt; null - hosted_zone_id = \u0026#34;Z2O1EMRO9K5GLX\u0026#34; -\u0026gt; null - id = \u0026#34;vus-first-bucket\u0026#34; -\u0026gt; null - object_lock_enabled = false -\u0026gt; null - region = \u0026#34;us-east-2\u0026#34; -\u0026gt; null - request_payer = \u0026#34;BucketOwner\u0026#34; -\u0026gt; null - tags = {} -\u0026gt; null - tags_all = {} -\u0026gt; null # (3 unchanged attributes hidden) - grant { - id = \u0026#34;32d5a6ff5e649dc794f25ddc46a8ef24c48bc2f7f85318544f20829b0721b334\u0026#34; -\u0026gt; null - permissions = [ - \u0026#34;FULL_CONTROL\u0026#34;, ] -\u0026gt; null - type = \u0026#34;CanonicalUser\u0026#34; -\u0026gt; null # (1 unchanged attribute hidden) } - server_side_encryption_configuration { - rule { - bucket_key_enabled = false -\u0026gt; null - apply_server_side_encryption_by_default { - sse_algorithm = \u0026#34;AES256\u0026#34; -\u0026gt; null # (1 unchanged attribute hidden) } } } - versioning { - enabled = false -\u0026gt; null - mfa_delete = false -\u0026gt; null } } Plan: 0 to add, 0 to change, 1 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes aws_s3_bucket.first_bucket: Destroying... [id=vus-first-bucket] aws_s3_bucket.first_bucket: Destruction complete after 0s Destroy complete! Resources: 1 destroyed. ","date":"29 May 2024","externalUrl":null,"permalink":"/posts/learning-terraform/configure_s3_bucket_terraform/","section":"Posts \u0026 Projects","summary":"Save AWS Access Keys for Terraform Admin Account # Create an AWS credentials file so that Terraform can get the programmatic credentials for the AWS administrator pushing configs via Terraform.","title":"How to Create an S3 Bucket in AWS Using Terraform","type":"posts"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/json/","section":"Tags","summary":"","title":"JSON","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/meraki/","section":"Tags","summary":"","title":"Meraki","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/network-administration/","section":"Tags","summary":"","title":"Network Administration","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/posts/networking-projects/","section":"Posts \u0026 Projects","summary":"","title":"Network Automation and Configuration Projects","type":"posts"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"","date":"29 May 2024","externalUrl":null,"permalink":"/tags/saml/","section":"Tags","summary":"","title":"SAML","type":"tags"},{"content":" This tutorial will guide you through the process of using Python to download the Layer 3 and Layer 7 rules from a Meraki MX into a CSV spreadsheet.\nWhat you Need # Access to the Meraki Dashboard\nA Meraki MX Appliance\nMeraki API Key\nBEFORE PROCEEDING, GENERATE YOUR API KEY AND SAVE IT SOMEWHERE SECURE\nGitHub Repo # You can either download the files from my Github\nor clone the repo:\ngit clone https://github.com/mikeovu/Meraki_Firewall_Rules_to_CSV.git Python Script # Here is the Python code that takes the firewall rules from the Meraki Dashboard and converts it into JSON \u0026ndash;\u0026gt; CSV.\nDo NOT run the code yet. Review it and then proceed to the Installation/Configuration section below\nIn the repo, it\u0026rsquo;s named security_to_csv.py:\nimport meraki import json import config import csv dashboard = meraki.DashboardAPI(config.api_key,single_request_timeout=999999) # Uses the Meraki library to grab Organization info. orgs = dashboard.organizations.getOrganizations() # Displays the org name and ID for org in orgs: print(\u0026#34;Organization name: \u0026#34; + str(org[\u0026#34;name\u0026#34;]) + \u0026#34; | \u0026#34; + \u0026#34;ID: \u0026#34; + str(org[\u0026#34;id\u0026#34;])) # Prompts user to select Organization ID org_id = input(\u0026#34;Select an organization id: \u0026#34;) # Displays dashboard orrganization mapped to organization ID networks = dashboard.organizations.getOrganizationNetworks(organizationId=org_id) # Displays all networks in the organization the user has selected for network in networks: print(\u0026#34;Network name: \u0026#34; + str(network[\u0026#34;name\u0026#34;]) + \u0026#34; | \u0026#34; + \u0026#34;ID: \u0026#34; + str(network[\u0026#34;id\u0026#34;])) print(\u0026#34;-------------------------------------\u0026#34;) breaker = \u0026#34;1\u0026#34; networks_to_output = [] # Promps user to specify network ids that they would like to see while breaker == \u0026#34;1\u0026#34;: net_id = input(\u0026#34;Continue to add network IDs that you want to receive L3 and L7 firewall rule outputs, enter 0 when finished\\n\u0026#34;) if net_id == \u0026#34;0\u0026#34;: breaker = \u0026#34;0\u0026#34; break networks_to_output.append(net_id) confirmation = input(\u0026#34;press y to proceed \u0026#34;) # Iterates through L7 rules of chosen network and creates a json file containing l7 policies for network in networks_to_output: with open(\u0026#39;firewall_rules/l7_rules.json\u0026#39;, \u0026#39;w\u0026#39;) as f: output_l7_response = dashboard.appliance.getNetworkApplianceFirewallL7FirewallRules(networkId=network) json.dump(output_l7_response, f, indent=2) # Iterates through L3 rules of chosen network and creates a json file containing L3 policies for network in networks_to_output: with open(\u0026#39;firewall_rules/l3_rules.json\u0026#39;, \u0026#39;w\u0026#39;) as f: output_l3_response = dashboard.appliance.getNetworkApplianceFirewallL3FirewallRules(networkId=network) json.dump(output_l3_response, f, indent=2) # Convert L7_rules.json to csv with open(\u0026#39;firewall_rules/l7_rules.json\u0026#39;) as l7_rules: l7_policies = json.load(l7_rules) policies = l7_policies[\u0026#39;rules\u0026#39;] # Open csv for writing l7 rules rule_data = open(\u0026#39;firewall_rules/l7_rules.csv\u0026#39;, \u0026#39;w\u0026#39;) # create the csv writer object csv_writer = csv.writer(rule_data) # Counter variable used for writing # headers to the csv file count = 0 for rule in policies: if count == 0: # Writing headers of CSV file header = rule.keys() csv_writer.writerow(header) count += 1 # Writing data of CSV file csv_writer.writerow(rule.values()) l7_rules.close() # Convert L3_rules.json to csv with open(\u0026#39;firewall_rules/l3_rules.json\u0026#39;) as l3_rules: l3_policies = json.load(l3_rules) policies = l3_policies[\u0026#39;rules\u0026#39;] # Open csv for writing l7 rules rule_data = open(\u0026#39;firewall_rules/l3_rules.csv\u0026#39;, \u0026#39;w\u0026#39;) # create the csv writer object csv_writer = csv.writer(rule_data) # Counter variable used for writing # headers to the csv file count = 0 for rule in policies: if count == 0: # Writing headers of CSV file header = rule.keys() csv_writer.writerow(header) count += 1 # Writing data of CSV file csv_writer.writerow(rule.values()) l3_rules.close() MX firewall rules you will be outputting to JSON and CSV # Here are the firewall rules specified in the Layer 3 and Layer 7 sections of the Security \u0026amp; SD-WAN \u0026gt; Configure \u0026gt; Firewall menu in the dashboard.\nInstallation/Configuration # Open the config.py file from the repository and paste in your Meraki API Key. api_key=\u0026#34;\u0026#34; install the dependencies required for the python script ~$ pip install -r requirements.txt run the python script ~$ python3 security_to_csv.py You will be asked to specify the Dashboard Organization you will be requesting firewall rules from. You will be asked to specify the network you will be requesting firewall rules from. Firewall Rules Directory # L3 Firewall Rules CSV \u0026amp; JSON # L7 Firewall Rules CSV \u0026amp; JSON # ","date":"29 May 2024","externalUrl":null,"permalink":"/posts/networking-projects/outputting_firewall_rules_to_dashboard/","section":"Posts \u0026 Projects","summary":"This tutorial will guide you through the process of using Python to download the Layer 3 and Layer 7 rules from a Meraki MX into a CSV spreadsheet.","title":"Using Python to Convert Meraki MX Firewall Rules into JSON and CSV Files","type":"posts"},{"content":" Configuration steps on AzureAD and Meraki Dashboards # This guide seems redundant, but there are some quirks that are not accounted for in the official Meraki documentation. I\u0026rsquo;ve included details to help avoid confusion during the configuration process\nFollow the steps in Meraki\u0026rsquo;s AnyConnect Azure AD SAML Configuration document. and stop at Step 7.\nBefore you start Step 7, navigate to your Meraki Dashboard and navigate to the Security \u0026amp; SD-WAN \u0026gt; Configure\u0026gt; Client VPN page.\nClick on AnyConnect Settings and select Enabled.\nCopy your hostname to your clipboard under the Client Connection Details page:\nNavigate back to Step 7 of the AnyConnect Azure AD SAML Configuration document. and follow the formatting instructions, using your hostname. mx-security-mmgjjktzjp.dynamic-m.com/saml/sp/metadata/SAML mx-security-mmgjjktzjp.dynamic-m.com/saml/sp/acs When you get to Step 9 of the document, make sure you add the hostname with https:// in front of it in the AnyConnect Server URL field: Navigate back to the AzureAD portal and click on the Single Sign-on menu.\nCheck the Basic SAML Configuration box to ensure you’ve entered the correct hostname in the Identifier (Entity ID) and Reply URL (Assertion Consumer Service URL) fields:\nGive AzureAD at least 30 minutes to accept the configuration. When I tested this, authentication kept failing within the first 30 minutes. After 30 minutes, I was able to authenticate without changing any settings. Test Client VPN on your End Device # If you did not set up an AnyConnect Profile for your end devices yet, follow the subsequent steps:\nOpen the AnyConnect Secure Mobility Client on your machine:\nCopy and paste the AnyConnect Server URL into the VPN field.\nWhen you press Connect, a Microsoft popup window will appear and prompt you to login using your credentials. ","date":"19 April 2023","externalUrl":null,"permalink":"/posts/networking-projects/configuring_anyconnect_azuread_saml/","section":"Posts \u0026 Projects","summary":"Configuration steps on AzureAD and Meraki Dashboards # This guide seems redundant, but there are some quirks that are not accounted for in the official Meraki documentation.","title":"How to Configure AzureAD SAML Authentication for AnyConnect VPN on Meraki","type":"posts"},{"content":"","date":"19 April 2023","externalUrl":null,"permalink":"/tags/vpn/","section":"Tags","summary":"","title":"VPN","type":"tags"},{"content":" Professional History # I have been an IT professional, specializing in network administration and design for a decade. Here\u0026rsquo;s an abridged story of how I went from a political science major in college to where I am now:\n\u0026#x1f3eb; 2005 - 2009: Majored in Political Science at UCLA and worked as an AV Technician for the student Union.\n\u0026#x1f3b5; 2006 - 2010: Toured around the country as a drummer in a band, where I obtained certain skills that would come in handy later in life. Some of the skills included building a social network in the music industry, face to face sales at malls (soliciting our album), and audio visual engineering (live sound and recording).\n\u0026#x1f4bb; 2011 - 2013: Learned the true value of a degree in political science \u0026#x1f4a9; This meant that I had to leverage my experience as a former college AV tech, as well as the skills I gained as a touring musician to obtain a job in corporate AV support. During my time as a corpoate AV support tech, I learned how to become an IT systems administrator, configuring and troubleshooting media servers, network switches, and end user machines. I eventually landed a job as a contractor at Google, which was a big milestone for me as an IT professional.\n\u0026#x1f4f6; 2014 - Present: I got a job at Coursea, where I was able to dive into network administration. This is where I decided to stick with network engineering as a long-term career path. I became a CCNA, worked in network operations for a few more years at a couple more companies - Cisco Meraki, Roche, and Stitch Fix. This experience lead to my current position as a pre-sales Solutions Engineer at Cisco Meraki\nSkills and Technology # Cisco Technologies - Where I gained knowledge and experience Routing, Switching, Wireless, Security, IoT Linux Administration - general systems administration for distributions like debian and ubuntu, and shell scripting\nWindows Administration - general Windows systems administration, powershell, and Active Directory Python - Language I use for network automation scripts.\nPostman - Tool I use for testing API endpoints.\nVisual Studio Code - My IDE of choice.\nGit - What I use to version control my projects. For example, managing the repo for this Hugo site.\nGithub - Where I store all of my project repositories.\nTerraform - The automation tool I like to use to deploy infrastructure.\nAWS - One of the cloud providers I have experience with.\nAzure - The other cloud provider I have experience with.\nCertifications # Logo Name Date Cisco Certified Network Associate (CCNA) Expires 2027 Cisco Certified DevNet Associate Expires 2027 Personal Life # My family and I love going on outdoor adventures. Not including the newest addition to the family for safety reasons.\nI\u0026rsquo;m an avid mountain biker and I love traveling with the purpose of exploring new trails.\nI\u0026rsquo;ve been a drummer for a couple of decades and used to be in a touring band.\n","date":"1 January 0001","externalUrl":null,"permalink":"/about/","section":"","summary":"Professional History # I have been an IT professional, specializing in network administration and design for a decade.","title":"About Me","type":"page"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/assurance/","section":"Tags","summary":"","title":"Assurance","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":" This is a high-level guide meant to describe how I created and deployed my static portfolio site. I\u0026rsquo;ve included links to resources you can use to create your own static site.\nCreate a Repo for your site in Github # To learn how to create your own repo, follow this guide\nI created my site\u0026rsquo;s repo.\nDownloading, Installing, and Using Hugo # Hugo is a web framework for building static sites.\nIt\u0026rsquo;s a great tool for people that don\u0026rsquo;t have the patience for traditional web design.\nThey have a ton of themes to choose from.\nI used the blowfish theme.\nEach theme has its own methods and techniques for customizing and generating content. I spent a lot of time on the blowfish docs to learn.\nWhy I used Hugo?\nI used Hugo because it allows you to generate content pages using Markdown.\nUse Git for version control # I used Git as my version control tool.\nHere\u0026rsquo;s the official reference page for Git.\nShipping my Site Using Azure Static Web Apps # Microsoft Azure has a Service called Static Web Apps\nIt automatically syncs with your site\u0026rsquo;s Github repo and maintains CI/CD using Github actions.\nI followed Microsoft\u0026rsquo;s official documentation to learn how to host my site.\nHere is the YAML workflow file:\nname: Azure Static Web Apps CI/CD on: push: branches: - main pull_request: types: [opened, synchronize, reopened, closed] branches: - main jobs: build_and_deploy_job: if: github.event_name == \u0026#39;push\u0026#39; || (github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action != \u0026#39;closed\u0026#39;) runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v3 with: submodules: true lfs: false - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AMBITIOUS_TREE_075E10E10 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;/\u0026#34; # App source code path api_location: \u0026#34;\u0026#34; # Api source code path - optional output_location: \u0026#34;public\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### close_pull_request_job: if: github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action == \u0026#39;closed\u0026#39; runs-on: ubuntu-latest name: Close Pull Request Job steps: - name: Close Pull Request id: closepullrequest uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_AMBITIOUS_TREE_075E10E10 }} action: \u0026#34;close\u0026#34; Any time you run a git push from your local repo, the changes will automatically update your shipped site.\n","date":"1 January 0001","externalUrl":null,"permalink":"/posts/how-i-made-this/","section":"Posts \u0026 Projects","summary":"This is a high-level guide meant to describe how I created and deployed my static portfolio site.","title":"How I Made this Website Using Hugo","type":"posts"},{"content":" Deploy Enterprise Agent # Objective # Download the Enterprise Agent Software and import it into the Virtualization Environment.\nFor this lab, I\u0026rsquo;m installing the enterprise agent on VirtualBox, running on my Intel NUC\nLogin to the ThousandEyes Dashboard # Access https://app.thousandeyes.com and login\nAdd New Enterprise Agent # In the left menu, navigate to Cloud \u0026amp; Enterprise Agents \u0026gt; Agent Settings\nClick on the Enterprise Agents tab and click Add New Enterprise Agent from the menu\nDownload The Virtual Appliance Software # Copy the Account Group Token and put it somewhere safe.\nChoose the Download - OVA button. Confirm the default download location in the Downloads folder by clicking Save\nOpen Oracle VM VirtualBox # Install the OVA # Click the Import button. Click the yellow folder icon on the right, navigate to the Downloads folder, and choose the downloaded OVA file. Click Open.\nChange Appliance Settings # In the Appliance settings section, edit the CPU field and change it to 2. At the bottom, under MAC Address Policy, click Generate new MAC addresses for all network adapters.\nClick Import\nClick the Start button to turn on the imported virtual machine. # Click into the virtual machine console. VirtualBox displays notifications about the configured host key, which allows you to leave the console windows. The host key in this lab environment is configured as Alt. Choose Do not show this message again and click the Capture button.\nSet Agent Hostname # From the menu, press N\nSet EA-LAB as the hostname and press Enter.\nConfigure IP addressing and DNS # For simplicity, keep DHCP for the IPv4 settings.\nAdd Account Group Token to Agent settings # Enter the IPv4 Address assigned to your Agent into a web browser.\nBypass the certificate warning to enter the configuration GUI.\nClick on the Agent tab and paste the Account Group Token saved earlier.\nGive the Agent some time to save the configuration.\nClick on the Review tab to see the status of the Agent.\n","date":"1 January 0001","externalUrl":null,"permalink":"/posts/thousandeyes_learning/deploy_enterprise_agent/","section":"Posts \u0026 Projects","summary":"Deploy Enterprise Agent # Objective # Download the Enterprise Agent Software and import it into the Virtualization Environment.","title":"How to Install an Enterprise Agent on a VM","type":"posts"},{"content":" Scheduling a Page Load Web Test # Pre-Requisites # A ThousandEyes Account\nWeb Layer - Page Load Tests # ThousandEyes Tests\nNavigating Waterfall Charts Documentation\nPage load tests use chromium, a browser based upon the Chromium browser codebase, to obtain in-browser site performance metrics. The metrics include the completed page load time and phase information for each DOM component.\nSchedule a Page Load Test # Navigate to https://app.thousandeyes.com\nNavigate to Cloud \u0026amp; Enterprise Agents \u0026gt; Test Settings\nIn the New Test Settings, select the Web Layer, Page Load Test Type and configure any name and description you want. On the top right hand corner, you will see the Views Enabled for This Test box to review the test parameters. In the Basic Configuration section, enter https://www.cisco.com in the URL field. From the Agents drop-down menu, we will select five Cloud Agents Leave all settings as default and click Run Once A new browser tab will open and display the page load test. Verify the test was successful by observing the Page Load Time parameter. Go back to the other tab with the test settings and set the interval to 5 minutes. Click Create New Test to schedule the test. Navigate to Cloud \u0026amp; Enterprise Agents \u0026gt; Views to view the test report. Examine the Test Report # In the Views section, click the Metrics drop-down menu, you will see all available metrics. By default, the test displays the most recent round of data. Remember that an interval of 5 minutes was set when configuring the test. Change the test round by clicking on another section of the timeline, or by navigating with the arrow buttons at the bottom of the timeline. The Views section allows you to observe historical test data for up to a month, even though Cisco ThousandEyes keeps some of the data longer.\nIn the bottom part of the test view are additional details on the metric you have chosen. Notice that there are three different tabs when the Page Load Time metric is chosen: Map, Table, Waterfall, and Dependent Applications. Observe the Map tab. Notice that this tab shows you two windows. The left window shows you average metrics from all agents and the right window shows agents on the world map. Agents are represented as circles in different colors, where colors are representing the performance of the Page Load Time metric.\nChoose the Table tab of the Page Load metric. This table gives you a comprehensive view of the test results from the individual agents that performed the test.\nChoose the Waterfall tab of the Page Load metric. Waterfall data is displayed if a specific agent is selected.\nThere is the Agent section with a drop-down menu located at the top of the page. Click the drop-down menu and choose one of the agents. Examine the waterfall view that appeared on the bottom of the page for the agent you have selected. This view can be used for determining which part of a web page is slowing things down. A developer can use this view and further optimize the web page.\nChange the view to HTTP Server and remove the filtered agent (if selected) by clicking next to the name of the Agent in the Agent drop-down menu. Notice that the Status by Phase section is shown when the Metrics parameter is set to Availability. The Status by Phase section displays the phases of the HTTP layer.\nChange the Metrics parameter to Response Time and open the Table tab. The Response Time column in the table is showing response times that are calculated by summing all other time values from the DNS Time, Connect Time, SSL Time, and Wait Time columns.\nChoose the Agent to Server view, choose Loss as metrics, open the Table tab, and observe the output. You can see how the network was behaving when the test was performed by observing the Error, Packet Loss, Latency, and Jitter columns.\nChoose the Path Visualization tab and observe the output at the bottom of the page. To see all network hops, move the two hop sliders together at the top-right corner of the Path Visualization view. Observe the topology. Notice that the utilized agents are shown on the left side of the page, and the destination server on the right side of the page. In the middle, you are presented with the different network paths that were used by the agents. Paths consist of network hops that are represented as small circles.\nHover your mouse cursor over a network hop and observe the output. Additional information about the network hop is provided, including its owner and the selected metric such as latency or packet drop. If a hop experienced any issues, the hop would turn its color to orange or red, making it easy to spot. Also notice that the full path that goes through this hop was highlighted, including all agents that were using it. Try different Highlight, Group, and Show options in the Path Visualization tab. You can select different parameters in the Group utility for troubleshooting. ","date":"1 January 0001","externalUrl":null,"permalink":"/posts/thousandeyes_learning/page-load/","section":"Posts \u0026 Projects","summary":"Scheduling a Page Load Web Test # Pre-Requisites # A ThousandEyes Account","title":"How to Schedule a Page Load Test in ThousandEyes","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/posts/thousandeyes_learning/","section":"Posts \u0026 Projects","summary":"","title":"Learning ThousandEyes","type":"posts"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/network-assurance/","section":"Tags","summary":"","title":"Network Assurance","type":"tags"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/tags/thousandeyes/","section":"Tags","summary":"","title":"ThousandEyes","type":"tags"}]